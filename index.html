<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="BRIGHT: Bi-level Feature Representation of Image Collections using Groups of Hash Tables">
  <meta name="keywords" content="Representation Learning, Generative Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>GALA</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script type="module" src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!--
  <link rel="icon" href="./static/images/favicon.svg">
  -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">GALA: Geometry-Aware Local Adaptive Grids for Detailed 3D Generation</h1>
            <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://santisy.github.io/">Dingdong Yang</a><sup>1</sup>,</span>
              <a href="https://actasidiot.github.io/">Yizhi Wang</a><sup>1</sup>,</span>
              <a href="https://prs.igp.ethz.ch/group/people/person-detail.schindler.html">Konrad Schindler</a><sup>2</sup>,</span>
              <a href="https://www.sfu.ca/~amahdavi/">Ali Mahdavi-Amiri</a><sup>1</sup>,</span>
              <a href="https://www.cs.sfu.ca/~haoz/">Hao Zhang</a><sup>1</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Simon Fraser University</span>
            <span class="author-block"><sup>2</sup>ETH Zurich</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2410.10037"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/santisy/project_sv"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/top_teaser_cropped.png"
           alt="concept of our method"
      >
      <h2 class="subtitle has-text-centered">
        Given a watertight mesh (a), our representation, <span class="dnerf">GALA</span>, 
        for geometry-aware local adaptive grids, distributes a set of root node voxels (<b><span style="color: rgb(235, 94, 51);">coral</span></b>) 
        to cover the mesh surfaces. An octree subdivision is applied to each root, 
        with a subset shown in (c). In each non-empty octree leaf node (<b><span style="color: rgb(50, 235, 128);">green</span></b>), 
        a local grid (<b><span style="color: rgb(255, 0, 0);">red dots</span></b>) is oriented and anisotropically scaled to adapt to and tightly 
        bound the local surface geometries. Only <b>277K parameters</b> with <b>8-bit</b> quantization 
        yields an accurate representation (e).
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          We propose GALA, a novel representation of 3D shapes that (i) excels at capturing
          and reproducing complex geometry and surface details, (ii) is computationally
          efficient, and (iii) lends itself to 3D generative modelling with modern, diffusion-
          based schemes. The key idea of GALA is to exploit both the global sparsity of
          surfaces within a 3D volume and their local surface properties. <strong>Sparsity</strong> is promoted
          by covering only the 3D object boundaries, not empty space, with an ensemble of
          tree root voxels. Each voxel contains an octree to further limit storage and compute
          to regions that contain surfaces. <strong>Adaptivity</strong> is achieved by fitting one local and
          geometry-aware coordinate frame in each non-empty leaf node. Adjusting the
          orientation of the local grid, as well as the anisotropic scales of its axes, to the local
          surface shape greatly increases the amount of detail that can be stored in a given
          amount of memory, which in turn allows for quantization without loss of quality.
          With our optimized C++/CUDA implementation, GALA can be fitted to an object
          in less than <strong>10 seconds</strong>. Moreover, the representation can efficiently be flattened
          and manipulated with transformer networks. We provide a cascaded generation
          pipeline capable of generating 3D shapes with great geometric detail.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="model-container">
          <model-viewer src="https://storage.googleapis.com/project-gala-3ds/out_0353_0.glb" alt="Model 3"
            camera-controls
            auto-rotate
            background-color="#ffffff"
            exposure="0.4"
            shadow-intensity="1">
          </model-viewer>
          <div class="model-title">Model 3</div>
        </div>
        <div class="model-container">
          <model-viewer src="https://storage.googleapis.com/project-gala-3ds/out_0353_0.glb" alt="Model 3"
            camera-controls
            auto-rotate
            background-color="#ffffff"
            exposure="0.4"
            shadow-intensity="1">
          </model-viewer>
          <div class="model-title">Model 3</div>
        </div>
        <div class="model-container">
          <model-viewer src="https://storage.googleapis.com/project-gala-3ds/out_0353_0.glb" alt="Model 3"
            camera-controls
            auto-rotate
            background-color="#ffffff"
            exposure="0.4"
            shadow-intensity="1">
          </model-viewer>
          <div class="model-title">Model 3</div>
        </div>
        <div class="model-container">
          <model-viewer src="https://storage.googleapis.com/project-gala-3ds/out_0353_0.glb" alt="Model 3"
            camera-controls
            auto-rotate
            background-color="#ffffff"
            exposure="0.4"
            shadow-intensity="1">
          </model-viewer>
          <div class="model-title">Model 3</div>
        </div>
        <div class="model-container">
          <model-viewer src="https://storage.googleapis.com/project-gala-3ds/out_0353_0.glb" alt="Model 3"
            camera-controls
            auto-rotate
            background-color="#ffffff"
            exposure="0.4"
            shadow-intensity="1">
          </model-viewer>
          <div class="model-title">Model 3</div>
        </div>
        <div class="model-container">
          <model-viewer src="https://storage.googleapis.com/project-gala-3ds/out_0353_0.glb" alt="Model 3"
            camera-controls
            auto-rotate
            background-color="#ffffff"
            exposure="0.4"
            shadow-intensity="1">
          </model-viewer>
          <div class="model-title">Model 3</div>
        </div>
      </div>
    </div>
      <h2 class="subtitle has-text-centered">
        Gallery of examples of detailed 3D generation results.
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <div class="column">
        <div class="content">
          <h2 class="title is-3">Pipeline </h2>
          <p>
            Overall pipeline of our method in three parts: Encoding, Hash Retrieval and Decoding.
          </p>
          <img src="static/images/pipeline.png">
        </div>
      </div>

      <!--
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>

      <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
      -->
    </div>
    <!--/ Matting. -->

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Results</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">Reconstruction</h3>
        <div class="content has-text-justified">
          <p>
            Our largest model (code size 16 × 16 × 16) has only 10M
more parameters (≈ 24% increase) than those of the corresponding VQGAN and RQ-VAE models.
However, our model outperforms VQGAN and RQ-VAE across all metrics, showing <b>55% and 41%
improvements</b> in LPIPS, respectively. In terms of computational costs, our method requires <b>≈ 50%
fewer GFlops </b>due to its smaller decoder and the efficiency gained from utilizing the hash tables.
          </p>
        </div>

        <div class="column">
          <div class="content">
            <img src="static/images/recon_results.png">
            <p align="center">
              Reconstruction metrics on the validation splits of FFHQ and LSUN-Church dataset.
            </p>
          </div>
        </div>
        <div class="column">
          <div class="content">
            <img src="static/images/size_of_models.png">
            <p align="center">
              Trainable parameters and computational load of decoders.
              An <sup>*</sup> indicates total number of parameters in hash tables and
              <sup>&Dagger;</sup> refers to total computational cost of decoding <em>and</em>
              feature retrieval from hash tables.
            </p>
          </div>
        </div>
        <br/>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <h3 class="title is-4">Generation</h3>
        <div class="content has-text-justified">
          <p>
            The <b>precision </b>of our
generated images is significantly higher than others, indicating a substantial reduction of low-quality
samples in our results, while our recall is almost the same as StyleGAN2. Our method also gets competitive results on
FID, CLIP-FID and Inception Scores.
          </p>
        </div>
        <div class="column">
          <div class="content">
            <img src="static/images/ffhq_gen_quant_results.png" class="center_table">
            <p align="center">
              Quantitative results of generation on FFHQ dataset. <sup>*</sup> denotes the results calculated 
              on publicly released checkpoint by LDM author on Github.
            </p>
          </div>
        </div>
        <div class="column">
          <div class="content">
            <img src="static/images/church_gen_quant_results.png" class="center_table">
            <p align="center">
              Quantitative results of generation on LSUN-Church dataset.
            </p>
          </div>
        </div>
        <!--/ Re-rendering. -->

        <!-- Nearest Neighbour -->
        <h3 class="title is-4">Nearest Neighbour</h3>
        <div class="content has-text-justified">
          <p>
            Although our method has much higher precision scores than previous methods, we show the nearest neighbour
            search by LPIPS to demonstrate that our generated samples are unique and <b>not</b> mere retrievals from the training dataset.
            In the following images, the <b>leftmost</b> images in each row are generated images from our method and the rest images in each row
            are the nearest neighbour search results.
          </p>
        </div>
        <div class="column">
          <div class="content_hscroll">
            <img src="static/images/ffhq_nearest_sup_show.png">
          </div>
        </div>
        <div class="column">
          <div class="content_hscroll">
            <img src="static/images/lsun_nearest_sup_show.png">
          </div>
        </div>
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!--/ Animation. -->



  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @article{yang2024gala,
        title={GALA: Geometry-Aware Local Adaptive Grids for Detailed 3D Generation},
        author={Yang, Dingdong and Wang, Yizhi and Schindler, Konrad and Amiri, Ali Mahdavi and Zhang, Hao},
        journal={arXiv preprint arXiv:2410.10037},
        year={2024}
      }
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
